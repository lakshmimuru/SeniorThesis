{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4bacc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "sys.path.append('../models/')\n",
    "sys.path.append('../dataloader/')\n",
    "from transformer_decoder import TransformerDecoder, PositionalEncoding\n",
    "from dataloader import low_rank\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3d9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, \n",
    "                 seed,\n",
    "                 dir_path, \n",
    "                 target_id,\n",
    "                 time_range,\n",
    "                 feature_dim,\n",
    "                 seq_len,\n",
    "                 cont_dim,\n",
    "                 lowrank_approx = False,\n",
    "                 sing_to_keep =55):\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        self.feature_dim = feature_dim\n",
    "        self.cont_dim = cont_dim\n",
    "        self.target_id = target_id\n",
    "        self.seq_len = seq_len\n",
    "        self.data_init = np.float32(np.load(dir_path+'data.npy',allow_pickle=True))\n",
    "        self.mask = np.load(dir_path+'mask.npy',allow_pickle=True).astype(bool)\n",
    "        self.data_init[self.mask] = 0\n",
    "        self.target_data = self.data_init[target_id] \n",
    "        red_data = np.delete(self.data_init,self.target_id,0)\n",
    "        if lowrank_approx:\n",
    "            red_data[:,:,:self.cont_dim] = low_rank(red_data[:,:,:self.cont_dim],sing_to_keep)\n",
    "            #fraction adjust estimator\n",
    "            data_min = np.amin(red_data.reshape(-1,self.feature_dim),0)[:self.cont_dim]\n",
    "            data_max = np.amax(red_data.reshape(-1,self.feature_dim),0)[:self.cont_dim]\n",
    "            self.data = np.insert(red_data,target_id,target_data,0)\t\n",
    "            self.data[:,:,:self.cont_dim] = (self.data[:,:,:self.cont_dim] - data_min)/(data_max - data_min)\n",
    "\n",
    "        else:\n",
    "            self.data_min = np.amin(red_data.reshape(-1,self.feature_dim),0)\n",
    "            self.data_max = np.amax(red_data.reshape(-1,self.feature_dim),0)\n",
    "            self.data = self.data_init\n",
    "            self.data[:,:,:self.cont_dim] = (self.data[:,:,:self.cont_dim] - self.data_min)/(self.data_max - self.data_min)\n",
    "        \n",
    "        self.seqs = self.data.shape[0]\n",
    "        self.time_range = time_range\n",
    "        self.time_ids = np.arange(self.time_range)\n",
    "    \n",
    "    def get_batch(self,batch_size):\n",
    "        \n",
    "        seqs = torch.zeros(batch_size,self.seq_len,self.feature_dim)\n",
    "    \n",
    "        for i in range(batch_size):\n",
    "            \n",
    "            seq_id = np.random.randint(self.seqs)\n",
    "            interv_time = np.random.randint(self.seq_len, self.time_range)\n",
    "            seq = self.data[seq_id,interv_time - self.seq_len:interv_time]\n",
    "            seqs[i] = torch.from_numpy(seq)\n",
    "\n",
    "        seqs = seqs.to(dtype=torch.float32)\n",
    "        return seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3579c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup dataloader\n",
    "seed  = 0\n",
    "dir_path  = '../datasets/synthetic_data_N_11_1/' \n",
    "target_id = 0\n",
    "time_range = 2000\n",
    "feature_dim = 2\n",
    "seq_len  = 256\n",
    "cont_dim = 2\n",
    "inp_feature = 2\n",
    "num_blocks = 3\n",
    "d_model = 32\n",
    "num_heads = 1\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-4\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07779097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, batch_size):\n",
    "    \n",
    "    warmup_steps = 5000\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),lr = lr, weight_decay = weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "                    optimizer,\n",
    "                    lambda steps: min((steps+1)/warmup_steps,1))\n",
    "    iters = 20000\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for i in range(iters):\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        seq = dataloader.get_batch(batch_size)\n",
    "        seq = seq.to(device)\n",
    "        pred = model(seq[:,:-1])\n",
    "        target = seq[:,1:].detach()\n",
    "        loss = criterion(pred,target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%5000 == 0:\n",
    "            print(f'Iters: {i}',loss.item())\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1f3aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for noise 0.5 and id 1\n",
      "Iters: 0 1.9597982168197632\n",
      "Iters: 5000 0.013714583590626717\n",
      "Iters: 10000 0.0054515814408659935\n",
      "Iters: 15000 0.0038313153199851513\n",
      "RMSE for noise 0.5 and 1 is 7.66731422052138\n",
      "Training for noise 0.5 and id 2\n",
      "Iters: 0 1.9669302701950073\n",
      "Iters: 5000 0.013386721722781658\n",
      "Iters: 10000 0.005320874508470297\n",
      "Iters: 15000 0.0036650991532951593\n",
      "RMSE for noise 0.5 and 2 is 7.582814516779606\n",
      "Training for noise 0.5 and id 3\n",
      "Iters: 0 1.9618945121765137\n",
      "Iters: 5000 0.013463648967444897\n",
      "Iters: 10000 0.005284096579998732\n",
      "Iters: 15000 0.0037042638286948204\n",
      "RMSE for noise 0.5 and 3 is 7.646450764781434\n",
      "Training for noise 0.5 and id 4\n",
      "Iters: 0 1.9768751859664917\n",
      "Iters: 5000 0.013210228644311428\n",
      "Iters: 10000 0.005325885023921728\n",
      "Iters: 15000 0.003645914141088724\n",
      "RMSE for noise 0.5 and 4 is 7.257323241551812\n",
      "Training for noise 1 and id 1\n",
      "Iters: 0 1.9539872407913208\n",
      "Iters: 5000 0.012496361508965492\n",
      "Iters: 10000 0.008739481680095196\n",
      "Iters: 15000 0.006585079710930586\n",
      "RMSE for noise 1 and 1 is 7.9467834059166425\n",
      "Training for noise 1 and id 2\n",
      "Iters: 0 1.9647303819656372\n",
      "Iters: 5000 0.011243315413594246\n",
      "Iters: 10000 0.008442615158855915\n",
      "Iters: 15000 0.005982179660350084\n",
      "RMSE for noise 1 and 2 is 7.711131529481646\n",
      "Training for noise 1 and id 3\n",
      "Iters: 0 1.9691158533096313\n",
      "Iters: 5000 0.011917585507035255\n",
      "Iters: 10000 0.009053903631865978\n",
      "Iters: 15000 0.006571130361407995\n",
      "RMSE for noise 1 and 3 is 8.587989197237803\n",
      "Training for noise 1 and id 4\n",
      "Iters: 0 1.9704054594039917\n",
      "Iters: 5000 0.010626701638102531\n",
      "Iters: 10000 0.008573068305850029\n",
      "Iters: 15000 0.006012775003910065\n",
      "RMSE for noise 1 and 4 is 8.64785653464247\n",
      "Training for noise 2 and id 1\n",
      "Iters: 0 1.9261342287063599\n",
      "Iters: 5000 0.014227520674467087\n",
      "Iters: 10000 0.012572145089507103\n",
      "Iters: 15000 0.01150081492960453\n",
      "RMSE for noise 2 and 1 is 6.46724427652192\n",
      "Training for noise 2 and id 2\n",
      "Iters: 0 1.926409125328064\n",
      "Iters: 5000 0.014321689493954182\n",
      "Iters: 10000 0.012804724276065826\n",
      "Iters: 15000 0.011318330653011799\n",
      "RMSE for noise 2 and 2 is 6.34178448861612\n",
      "Training for noise 2 and id 3\n",
      "Iters: 0 1.9281237125396729\n",
      "Iters: 5000 0.014599368907511234\n",
      "Iters: 10000 0.013088884763419628\n",
      "Iters: 15000 0.011644311249256134\n",
      "RMSE for noise 2 and 3 is 6.346883192553942\n",
      "Training for noise 2 and id 4\n",
      "Iters: 0 1.9132264852523804\n",
      "Iters: 5000 0.015214311890304089\n",
      "Iters: 10000 0.013780688866972923\n",
      "Iters: 15000 0.012212044559419155\n",
      "RMSE for noise 2 and 4 is 6.5020904164234645\n"
     ]
    }
   ],
   "source": [
    "#training for syntehtic+noise \n",
    "\n",
    "ids = [1,2,3,4]\n",
    "noise = [0.5,1,2]\n",
    "for n in noise:\n",
    "    for id in ids:\n",
    "        op_dir = f'../logs_dir/txf_baseline/noise_{n}_{id}/'\n",
    "        if not os.path.exists(op_dir):\n",
    "            os.makedirs(op_dir)\n",
    "        \n",
    "        datapath = f'../datasets/synthetic_noise{n}_{id}/'\n",
    "        dataloader = DataLoader(seed,\n",
    "                 datapath, \n",
    "                 target_id,\n",
    "                 time_range,\n",
    "                 feature_dim,\n",
    "                 seq_len,\n",
    "                 cont_dim,\n",
    "                lowrank_approx=False,\n",
    "                    )\n",
    "        model = TransformerDecoder(inp_feature, num_blocks, seq_len, d_model, num_heads)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(f'Training for noise {n} and id {id}')\n",
    "        model = train_model(model, dataloader, 64)\n",
    "        #save model\n",
    "        torch.save(model.state_dict(),op_dir+'model.pt')\n",
    "        #generating\n",
    "\n",
    "        interv_time = 1600\n",
    "        target_data = dataloader.target_data\n",
    "        data_min = dataloader.data_min\n",
    "        data_max = dataloader.data_max\n",
    "        target_data = (target_data - data_min )/(data_max - data_min)\n",
    "        target_data = torch.from_numpy(target_data).unsqueeze(0)\n",
    "        target_data = target_data.to(device)\n",
    "        op = model.generate(target_data[:,:interv_time],2000)\n",
    "        op = op.cpu().numpy()\n",
    "        mean = np.load(datapath+'mean1.npy')\n",
    "        test_mean = mean[0,1600:]\n",
    "        op = op*(data_max - data_min) + data_min\n",
    "        pred_mean = np.squeeze(op[:,interv_time:,0])\n",
    "        error_pred = np.sqrt(np.mean((pred_mean - test_mean )**2))\n",
    "        print(f'RMSE for noise {n} and {id} is {error_pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2b202d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for donors 25 and id 1\n",
      "Iters: 0 1.9632132053375244\n",
      "Iters: 5000 0.010701916180551052\n",
      "Iters: 10000 0.008230019360780716\n",
      "Iters: 15000 0.005717776250094175\n",
      "RMSE for donors 25 and 1 is 8.914217948913574\n",
      "Training for donors 25 and id 2\n",
      "Iters: 0 1.956062912940979\n",
      "Iters: 5000 0.011235468089580536\n",
      "Iters: 10000 0.008562842383980751\n",
      "Iters: 15000 0.006131433416157961\n",
      "RMSE for donors 25 and 2 is 8.509750366210938\n",
      "Training for donors 25 and id 3\n",
      "Iters: 0 1.9551167488098145\n",
      "Iters: 5000 0.011441150680184364\n",
      "Iters: 10000 0.008514096029102802\n",
      "Iters: 15000 0.006125223822891712\n",
      "RMSE for donors 25 and 3 is 8.719613075256348\n",
      "Training for donors 50 and id 1\n",
      "Iters: 0 1.9521926641464233\n",
      "Iters: 5000 0.010715262033045292\n",
      "Iters: 10000 0.00833373237401247\n",
      "Iters: 15000 0.005651321727782488\n",
      "RMSE for donors 50 and 1 is 7.692732810974121\n",
      "Training for donors 50 and id 2\n",
      "Iters: 0 1.9621872901916504\n",
      "Iters: 5000 0.010569652542471886\n",
      "Iters: 10000 0.007994486019015312\n",
      "Iters: 15000 0.005419246852397919\n",
      "RMSE for donors 50 and 2 is 9.147173881530762\n",
      "Training for donors 50 and id 3\n",
      "Iters: 0 1.951269268989563\n",
      "Iters: 5000 0.011681385338306427\n",
      "Iters: 10000 0.008508880622684956\n",
      "Iters: 15000 0.006117516662925482\n",
      "RMSE for donors 50 and 3 is 7.64719295501709\n",
      "Training for donors 75 and id 1\n",
      "Iters: 0 1.9465463161468506\n",
      "Iters: 5000 0.011028540320694447\n",
      "Iters: 10000 0.008081400766968727\n",
      "Iters: 15000 0.005616998299956322\n",
      "RMSE for donors 75 and 1 is 10.749821662902832\n",
      "Training for donors 75 and id 2\n",
      "Iters: 0 1.9399429559707642\n",
      "Iters: 5000 0.010012797079980373\n",
      "Iters: 10000 0.007683488540351391\n",
      "Iters: 15000 0.005182710476219654\n",
      "RMSE for donors 75 and 2 is 10.06509780883789\n",
      "Training for donors 75 and id 3\n",
      "Iters: 0 1.9526442289352417\n",
      "Iters: 5000 0.011145583353936672\n",
      "Iters: 10000 0.008083945140242577\n",
      "Iters: 15000 0.0057418919168412685\n",
      "RMSE for donors 75 and 3 is 11.023176193237305\n",
      "Training for donors 100 and id 1\n",
      "Iters: 0 1.9429258108139038\n",
      "Iters: 5000 0.0100770378485322\n",
      "Iters: 10000 0.008016799576580524\n",
      "Iters: 15000 0.005537502933293581\n",
      "RMSE for donors 100 and 1 is 9.150805473327637\n",
      "Training for donors 100 and id 2\n",
      "Iters: 0 1.9461699724197388\n",
      "Iters: 5000 0.009895769879221916\n",
      "Iters: 10000 0.007765442598611116\n",
      "Iters: 15000 0.005331068765372038\n",
      "RMSE for donors 100 and 2 is 9.622058868408203\n",
      "Training for donors 100 and id 3\n",
      "Iters: 0 1.9403618574142456\n",
      "Iters: 5000 0.010029551573097706\n",
      "Iters: 10000 0.007724619470536709\n",
      "Iters: 15000 0.005282433703541756\n",
      "RMSE for donors 100 and 3 is 8.798208236694336\n"
     ]
    }
   ],
   "source": [
    "#training for syntehtic+donors \n",
    "\n",
    "ids = [1,2,3]\n",
    "donors = [26,51,76,101]\n",
    "for n in donors:\n",
    "    for id in ids:\n",
    "        op_dir = f'../logs_dir/txf_baseline/donors_{n}_{id}/'\n",
    "        if not os.path.exists(op_dir):\n",
    "            os.makedirs(op_dir)\n",
    "        \n",
    "        datapath = f'../datasets/synthetic_data_N_{n}_{id}/'\n",
    "        dataloader = DataLoader(seed,\n",
    "                 datapath, \n",
    "                 target_id,\n",
    "                 time_range,\n",
    "                 feature_dim,\n",
    "                 seq_len,\n",
    "                 cont_dim,\n",
    "                lowrank_approx=False,\n",
    "                    )\n",
    "        model = TransformerDecoder(inp_feature, num_blocks, seq_len, d_model, num_heads)\n",
    "        model = model.to(device)\n",
    "        \n",
    "        print(f'Training for donors {n-1} and id {id}')\n",
    "        model = train_model(model, dataloader, 64)\n",
    "        #save model\n",
    "        torch.save(model.state_dict(),op_dir+'model.pt')\n",
    "        #generating\n",
    "\n",
    "        interv_time = 1600\n",
    "        target_data = dataloader.target_data\n",
    "        data_min = dataloader.data_min\n",
    "        data_max = dataloader.data_max\n",
    "        target_data = (target_data - data_min )/(data_max - data_min)\n",
    "        target_data = torch.from_numpy(target_data).unsqueeze(0)\n",
    "        target_data = target_data.to(device)\n",
    "        op = model.generate(target_data[:,:interv_time],2000)\n",
    "        op = op.cpu().numpy()\n",
    "        mean = np.load(datapath+'mean1.npy')\n",
    "        test_mean = mean[0,1600:]\n",
    "        op = op*(data_max - data_min) + data_min\n",
    "        pred_mean = np.squeeze(op[:,interv_time:,0])\n",
    "        error_pred = np.sqrt(np.mean((pred_mean - test_mean )**2))\n",
    "        print(f'RMSE for donors {n-1} and {id} is {error_pred}')\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
